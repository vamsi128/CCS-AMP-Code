{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS-AMP for unsourced multiple access\n",
    "\n",
    "This notebook contains CCS-AMP encoder/decoder for unsourced multiple access using Hadamard design matrices.\n",
    "\n",
    "The proposed algorithm goes back and forth between inner AMP and outer tree decoding components.\n",
    "\n",
    "The code is based on the following articles:\n",
    "\n",
    "A coded compressed sensing scheme for uncoordinated multiple access, available @ https://arxiv.org/pdf/1809.04745.pdf\n",
    "\n",
    "SPARCs for Unsourced Random Access, available @ https://arxiv.org/abs/1901.06234\n",
    "\n",
    "On Approximate Message Passing for Unsourced Access with Coded Compressed Sensing, available @ https://arxiv.org/pdf/2001.03705.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "# from pyfht import block_sub_fht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fht(u):\n",
    "    \"\"\"\n",
    "    Perform fast Hadamard transform of u, in-place.\n",
    "    Note len(u) must be a power of two.\n",
    "    \"\"\"\n",
    "    N = len(u)\n",
    "    i = N>>1\n",
    "    while i:\n",
    "        for j in range(N):\n",
    "            if (i&j) == 0:\n",
    "                temp = u[j]\n",
    "                u[j] += u[i|j]\n",
    "                u[i|j] = temp - u[i|j]\n",
    "        i>>= 1\n",
    "\n",
    "def sub_fht(n, m, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    Returns functions to compute the sub-sampled Walsh-Hadamard transform,\n",
    "    i.e., operating with a wide rectangular matrix of random +/-1 entries.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns\n",
    "\n",
    "    It is most efficient (but not required) for max(m,n+1) to be a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional n-long array of row indices in [1, max(m,n)] to\n",
    "              implement subsampling; generated by seed if not specified,\n",
    "              but may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length m\n",
    "        Ay(y): computes A'.y (of length m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    if new_embedding:\n",
    "        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "    else:\n",
    "        w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (n,)\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        rng.shuffle(idxs)\n",
    "        ordering = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == m, \"x must be m long\"\n",
    "        y = np.zeros(w)\n",
    "        if new_embedding:\n",
    "            y[w-m:] = x.reshape(m)\n",
    "        else:\n",
    "            y[:m] = x.reshape(m)\n",
    "        fht(y)\n",
    "        return y[ordering]\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n, \"input must be n long\"\n",
    "        x = np.zeros(w)\n",
    "        x[ordering] = y.reshape(n)\n",
    "        fht(x)\n",
    "        if new_embedding:\n",
    "            return x[w-m:]\n",
    "        else:\n",
    "            return x[:m]\n",
    "\n",
    "    return Ax, Ay, ordering\n",
    "\n",
    "def block_sub_fht(n, m, l, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially\n",
    "    offering substantial speed improvements.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns per block\n",
    "    l: number of blocks\n",
    "\n",
    "    It is most efficient (though not required) when max(m,n+1) is a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to\n",
    "              implement subsampling; generated by seed if not specified, but\n",
    "              may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length l*m\n",
    "        Ay(y): computes A'.y (of length l*m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    assert l > 0, \"l must be positive\"\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (l, n)\n",
    "    else:\n",
    "        if new_embedding:\n",
    "            w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "        else:\n",
    "            w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        ordering = np.empty((l, n), dtype=np.uint32)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        for ll in range(l):\n",
    "            rng.shuffle(idxs)\n",
    "            ordering[ll] = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == l*m\n",
    "        out = np.zeros(n)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out += ax(x[ll*m:(ll+1)*m])\n",
    "        return out\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n\n",
    "        out = np.empty(l*m)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out[ll*m:(ll+1)*m] = ay(y)\n",
    "        return out\n",
    "\n",
    "    return Ax, Ay, ordering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree encoder\n",
    "\n",
    "This function encodes the payloads corresponding to users into codewords from the specified tree code. \n",
    "\n",
    "Parity bits in section $i$ are generated based on the information sections $i$ is connected to\n",
    "\n",
    "Computations are done within the ring of integers modulo length of the section to enable FFT-based BP on the outer graph\n",
    "\n",
    "This function outputs the sparse representation of encoded messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_encode(tx_message,K,messageBlocks,G,L,J):\n",
    "    encoded_tx_message = np.zeros((K,L),dtype=int)\n",
    "    \n",
    "    encoded_tx_message[:,0] = tx_message[:,0:J].dot(2**np.arange(J)[::-1])\n",
    "    for i in range(1,L):\n",
    "        if messageBlocks[i]:\n",
    "            # copy the message if i is an information section\n",
    "            encoded_tx_message[:,i] = tx_message[:,np.sum(messageBlocks[:i])*J:(np.sum(messageBlocks[:i])+1)*J].dot(2**np.arange(J)[::-1])\n",
    "        else:\n",
    "            # compute the parity if i is a parity section\n",
    "            indices = np.where(G[i])[0]\n",
    "            ParityInteger=np.zeros((K,1),dtype='int')\n",
    "            for j in indices:\n",
    "                ParityInteger1 = encoded_tx_message[:,j].reshape(-1,1)\n",
    "                ParityInteger = np.mod(ParityInteger+ParityInteger1,2**J)\n",
    "            encoded_tx_message[:,i] = ParityInteger.reshape(-1)\n",
    "    \n",
    "    return encoded_tx_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts message indices into $L$-sparse vectors of length $L 2^J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices_to_sparse(encoded_tx_message_indices,L,J,K):\n",
    "    encoded_tx_message_sparse=np.zeros((L*2**J,1),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = encoded_tx_message_indices[:,i]\n",
    "        B = A.reshape([-1,1])\n",
    "        np.add.at(encoded_tx_message_sparse, i*2**J+B, 1)        \n",
    "    return encoded_tx_message_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the index representation corresponding to a SPARC-like vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_to_indices(cs_decoded_tx_message_sparse,L,J,listSize):\n",
    "    cs_decoded_tx_message = np.zeros((listSize,L),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = cs_decoded_tx_message_sparse[i*2**J:(i+1)*2**J]\n",
    "        idx = (A.reshape(2**J,)).argsort()[np.arange(2**J-listSize)]\n",
    "        B = np.setdiff1d(np.arange(2**J),idx)\n",
    "        cs_decoded_tx_message[:,i] = B \n",
    "\n",
    "    return cs_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information bits from retained paths in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_indices(Paths,cs_decoded_tx_message, L,J):\n",
    "    msg_bits = np.empty(shape=(0,0))\n",
    "    L1 = Paths.shape[0]\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        msg_bits = np.vstack((msg_bits,msg_bit)) if msg_bits.size else msg_bit           \n",
    "    return msg_bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Codebook\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A.\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\cdot B$ and $z^T\\cdot A$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparc_codebook(L, M, n,P):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1)/ np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1)/ np.sqrt(n) \n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Approximation\n",
    "\n",
    "This function outputs the closest approximation to the input vector given that its L1 norm is 1 and no entry is greater than 1/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximateVector(x, K):    \n",
    "\n",
    "    # normalize initial value of x\n",
    "    xOrig = x / np.linalg.norm(x, ord=1)\n",
    "    \n",
    "    # create vector to hold best approximation of x\n",
    "    xHt = xOrig.copy()\n",
    "    u = np.zeros(len(xHt))\n",
    "    \n",
    "    # run approximation algorithm\n",
    "    while np.amax(xHt) > (1/K):\n",
    "        minIndices = np.argmin([(1/K)*np.ones(xHt.shape), xHt], axis=0)\n",
    "        xHt = np.min([(1/K)*np.ones(xHt.shape), xHt], axis=0)\n",
    "        \n",
    "        deficit = 1 - np.linalg.norm(xHt, ord=1)\n",
    "        \n",
    "        if deficit > 0:\n",
    "            mIxHtNorm = np.linalg.norm((xHt*minIndices), ord=1)\n",
    "            scaleFactor = (deficit + mIxHtNorm) / mIxHtNorm\n",
    "            xHt = scaleFactor*(minIndices*xHt) + (1/K)*(np.ones(xHt.shape) - minIndices)\n",
    "\n",
    "    # return admissible approximation of x\n",
    "    return xHt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP on outer graph\n",
    "\n",
    "This function computes the priors on the unknown sparse vector, given effective obervations of the (graph) neighboring sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePrior(s,G,messageBlocks,L,M,p0,K,τ,Phat,numBPiter,performApproximation):\n",
    "    \n",
    "    q = np.zeros(s.shape,dtype=float)\n",
    "    p1 = p0*np.ones(s.shape,dtype=float)\n",
    "    \n",
    "    for iter in range(numBPiter):\n",
    "        \n",
    "        # Translate the effective observation into PME. For the first iteration of BP, use the uninformative prior p0\n",
    "        temp_beta = (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-p1)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "        # Reshape PME into an LxM matrix\n",
    "        Beta = temp_beta.reshape(L,-1)\n",
    "        #print(Beta.shape,np.sum(Beta,axis=1))\n",
    "        Beta = Beta/(np.sum(Beta,axis=1).reshape(L,-1))\n",
    "        # Rotate PME 180deg about y-axis\n",
    "        Betaflipped = np.hstack((Beta[:,0].reshape(-1,1),np.flip(Beta[:,1:],axis=1)))\n",
    "        # Compute and store all FFTs\n",
    "        BetaFFT = np.fft.fft(Beta)\n",
    "        BetaflippedFFT = np.fft.fft(Betaflipped)\n",
    "        for i in range(L):\n",
    "            if messageBlocks[i]:\n",
    "                # Parity sections connected to info section i\n",
    "                parityIndices = np.where(G[i])[0]\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                for j in parityIndices:\n",
    "                    # Other info blocks connected to this parity block\n",
    "                    messageIndices = np.setdiff1d(np.where(G[j])[0],i)\n",
    "                    BetaFFTprime = np.vstack((BetaFFT[j],BetaflippedFFT[messageIndices,:]))\n",
    "                    # Multiply the relevant FFTs\n",
    "                    BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                    # IFFT\n",
    "                    BetaIFFTprime1 = np.fft.ifft(BetaFFTprime).real\n",
    "                    BetaIFFTprime = np.vstack((BetaIFFTprime,BetaIFFTprime1)) if BetaIFFTprime.size else BetaIFFTprime1\n",
    "                BetaIFFTprime = np.prod(BetaIFFTprime,axis=0)\n",
    "            else:\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                # Information sections connected to this parity section (assuming no parity over parity sections)\n",
    "                Indices = np.where(G[i])[0]\n",
    "                # FFT\n",
    "                BetaFFTprime = BetaFFT[Indices,:]\n",
    "                # Multiply the relevant FFTs\n",
    "                BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                # IFFT\n",
    "                BetaIFFTprime = np.fft.ifft(BetaFFTprime).real\n",
    "            \n",
    "            # Normalize to ensure it sums to one\n",
    "            # p1[i*M:(i+1)*M] = (BetaIFFTprime/np.sum(BetaIFFTprime)).reshape(-1,1)            \n",
    "            # p1[i*M:(i+1)*M]  = 1-(1-p1[i*M:(i+1)*M] )**K \n",
    "            if performApproximation: \n",
    "                # p1[i*M:(i+1)*M] = approximateVector(p1[i*M:(i+1)*M].ravel(), K).reshape((M, 1))\n",
    "                p1[i*M:(i+1)*M] = approximateVector(BetaIFFTprime.ravel(), K).reshape((M, 1))\n",
    "            else:\n",
    "                p1[i*M:(i+1)*M] = (BetaIFFTprime/np.sum(BetaIFFTprime)).reshape(-1,1)            \n",
    "                p1[i*M:(i+1)*M]  = 1-(1-p1[i*M:(i+1)*M] )**K \n",
    "                \n",
    "            \n",
    "    q = np.minimum(p1,1)          \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but note we use `longdouble` types because the expentials are often too big to fit into a normal `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,performApproximation):\n",
    "\n",
    "    n = y.size\n",
    "    β = np.zeros((L*M, 1))\n",
    "    z = y\n",
    "    Phat = n*P/L\n",
    "    # Store the values of τ corresponding to each iteration\n",
    "    τ_evolution = np.zeros((T,1))\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        # Compute τ online using the residual\n",
    "        τ = np.sqrt(np.sum(z**2)/n)\n",
    "        \n",
    "        # effective observation\n",
    "        s = (np.sqrt(Phat)*β + Az(z)).astype(np.longdouble)\n",
    "        \n",
    "        if BPonOuterGraph==0:\n",
    "            # Use the uninformative prior p0 for Giuseppe's scheme\n",
    "            q = p0\n",
    "        else:\n",
    "            # Compute the prior through BP on outer graph\n",
    "            q = computePrior(s,G,messageBlocks,L,M,p0,K,τ,Phat,numBPiter,performApproximation)\n",
    "            \n",
    "        # denoiser\n",
    "        β = (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-q)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "        \n",
    "        # residual\n",
    "        z = y - np.sqrt(Phat)*Ab(β) + (z/(n*τ**2)) * (Phat*np.sum(β) - Phat*np.sum(β**2))\n",
    "        τ_evolution[t] = τ\n",
    "\n",
    "    return β,τ_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=4      # Number of active users (sparsity level of index vector)\n",
    "B=32     # Payload size of each active user (number of bits each active device wants to transmit)\n",
    "L=8      # Number of sections/sub-blocks\n",
    "J=8      # Length of each coded sub-block\n",
    "n=1536   # Total number of channel uses (real d.o.f) - used for appropriate power calculations (N = L*K*J*lambda=6)\n",
    "T=6      # Number of AMP iterations\n",
    "M=2**J   # Length of each section\n",
    "messageBlocks = np.array([1,1,1,1,0,0,0,0]).astype(int) # Indicates the indices of information blocks\n",
    "G = np.zeros((L,L)).astype(int) # Adjacency matrix of the outer code/graph\n",
    "# G contains info on what parity blocks a message is attached to and what message blocks a parity is involved with\n",
    "# Currently, we do not allow parity over parities. BP code needs to be modified a little to accomodate parity over parities\n",
    "G[0, [4, 6]] = 1  # 0, 1, 2, 3 info\n",
    "G[1, [5, 6]] = 1  # 4, 5, 6, 7 parity \n",
    "G[2, [4, 7]] = 1\n",
    "G[3, [5, 7]] = 1\n",
    "G[4, [0, 2]] = 1\n",
    "G[5, [1, 3]] = 1\n",
    "G[6, [0, 1]] = 1\n",
    "G[7, [2, 3]] = 1\n",
    "\n",
    "BPonOuterGraph = 1 # Indicates if BP is allowed on the outer code.Setting this to zero defaults back to Giuseppe's scheme that uses uninformative prior\n",
    "numBPiter = 1; # Number of BP iterations on outer code. 1 seems to be good enough & AMP theory including state evolution valid only for one BP iteration\n",
    "# EbNodB = 6.5 # Energy per bit. With iterative extension, operating EbN0 falls to 2.05 dB for 25 users with 1 round SIC\n",
    "\n",
    "# function that runs simulation\n",
    "def simulate(EbNodB, k, performApproximation):\n",
    "    K = k\n",
    "    listSize = 4*K       # List size retained for each section after AMP converges\n",
    "    p0 = 1-(1-1/M)**K    # Giuseppe's uninformative prior\n",
    "    delta = 0\n",
    "    maxSims=10           # number of simulations\n",
    "    \n",
    "    # EbN0 in linear scale\n",
    "    EbNo = 10**(EbNodB/10)\n",
    "    P = 2*B*EbNo/n\n",
    "    σ_n = 1\n",
    "    \n",
    "    # We assume equal power allocation for all the sections. Code has to be modified a little to accomodate non-uniform power allocations\n",
    "    Phat = n*P/L\n",
    "\n",
    "    # Variables used for computing Mean-Squared Error\n",
    "    msgDetected=0\n",
    "    mserrs = np.zeros(maxSims)\n",
    "\n",
    "    for sims in range(maxSims):\n",
    "\n",
    "        # Generate active users message sequences\n",
    "        tx_message = np.random.randint(2, size=(K,B))\n",
    "\n",
    "        # Outer-encode the message sequences\n",
    "        encoded_tx_message_indices = Tree_encode(tx_message,K,messageBlocks,G,L,J)\n",
    "\n",
    "        # Convert indices to sparse representation\n",
    "        β_0 = convert_indices_to_sparse(encoded_tx_message_indices,L,J,K)\n",
    "\n",
    "        # Generate the binned SPARC codebook\n",
    "        Ab, Az = sparc_codebook(L, M, n, P)\n",
    "\n",
    "        # Generate our transmitted signal X\n",
    "        x = np.sqrt(Phat)*Ab(β_0)\n",
    "\n",
    "        # Generate random channel noise and thus also received signal y\n",
    "        z = np.random.randn(n, 1) * σ_n\n",
    "        y = (x + z).reshape(-1, 1)\n",
    "\n",
    "        # Run AMP decoding\n",
    "        β, τ_evolution = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,performApproximation)        \n",
    "        \n",
    "        # Compute Mean-squared error for current simulation\n",
    "        mserrs[sims] = (1/(n*P)) * np.linalg.norm(β-β_0)**2;\n",
    "\n",
    "    mse = sum(mserrs) / maxSims\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE vs SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeSNR = np.array([2, 4, 6, 8, 10, 12])\n",
    "data = np.zeros((2, len(rangeSNR)))\n",
    "K = 4\n",
    "\n",
    "for idxSim in range(2):\n",
    "    for idxSnr in range(len(rangeSNR)):\n",
    "        print(f'Running simulation {(idxSim)*(len(rangeSNR))+idxSnr+1}/{2*len(rangeSNR)}')\n",
    "        data[idxSim, idxSnr] = simulate(rangeSNR[idxSnr], K, idxSim)\n",
    "        \n",
    "print(data)\n",
    "\n",
    "# Plot results of MSE vs SNR\n",
    "plt.figure(1)\n",
    "plt.plot(rangeSNR, data[0,:], 'b', label='No Normalization')\n",
    "plt.plot(rangeSNR, data[1, :], 'r', label='With Normalization')\n",
    "plt.legend()\n",
    "plt.xlabel('Eb/No')\n",
    "plt.ylabel(r'$\\frac{1}{nP}||\\beta-\\beta_{0}||^2$')\n",
    "plt.title(r'MSE of $\\beta$ with $K=4$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE vs Sparsity Level (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsityParams = np.array([2, 4, 8])\n",
    "data = np.zeros((2, len(sparsityParams)))\n",
    "EbNo = 8\n",
    "\n",
    "for idxSim in range(2):\n",
    "    for idxK in range(len(sparsityParams)):\n",
    "        print(f'Running simulation {(idxSim*len(sparsityParams)) + idxK + 1}/{2*len(sparsityParams)}')\n",
    "        data[idxSim, idxK] = simulate(EbNo, sparsityParams[idxK], idxSim)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Plot results of MSE vs Sparsity level (K)\n",
    "plt.figure(2)\n",
    "plt.semilogy(sparsityParams / M, data[0,:], 'b', label='No Normalization')\n",
    "plt.semilogy(sparsityParams / M, data[1,:], 'r', label='With Normalization')\n",
    "plt.legend()\n",
    "plt.xlabel(r'Sparsity Percentage')\n",
    "plt.ylabel(r'$\\frac{1}{nP}||\\beta-\\beta_{0}||^2$')\n",
    "plt.title(r'MSE of $\\beta$ vs $K$ with $\\frac{E_b}{N_0} = 8$dB')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of Approximation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of approximation script\n",
    "testVec = np.array([0.4, 0.3, 0.05, 0.07, 0.03, 0.1, 0.05, 0, 0, 0])\n",
    "k = 4\n",
    "\n",
    "testVecHt = approximateVector(testVec, k)\n",
    "\n",
    "print(testVec)\n",
    "print(testVecHt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
