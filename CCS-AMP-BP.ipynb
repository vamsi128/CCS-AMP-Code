{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS-AMP for unsourced multiple access\n",
    "\n",
    "This notebook contains CCS-AMP encoder/decoder for unsourced multiple access using Hadamard design matrices.\n",
    "\n",
    "The proposed algorithm goes back and forth between inner AMP and outer tree decoding components.\n",
    "\n",
    "The code is based on the following articles:\n",
    "\n",
    "A coded compressed sensing scheme for uncoordinated multiple access, available @ https://arxiv.org/pdf/1809.04745.pdf\n",
    "\n",
    "SPARCs for Unsourced Random Access, available @ https://arxiv.org/abs/1901.06234\n",
    "\n",
    "On Approximate Message Passing for Unsourced Access with Coded Compressed Sensing, available @ https://arxiv.org/pdf/2001.03705.pdf\n",
    "\n",
    "Numbered equations are from https://arxiv.org/pdf/2010.04364.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fht(u):\n",
    "    \"\"\"\n",
    "    Perform fast Hadamard transform of u, in-place.\n",
    "    Note len(u) must be a power of two.\n",
    "    \"\"\"\n",
    "    N = len(u)\n",
    "    i = N>>1\n",
    "    while i:\n",
    "        for j in range(N):\n",
    "            if (i&j) == 0:\n",
    "                temp = u[j]\n",
    "                u[j] += u[i|j]\n",
    "                u[i|j] = temp - u[i|j]\n",
    "        i>>= 1\n",
    "\n",
    "def sub_fht(n, m, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    Returns functions to compute the sub-sampled Walsh-Hadamard transform,\n",
    "    i.e., operating with a wide rectangular matrix of random +/-1 entries.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns\n",
    "\n",
    "    It is most efficient (but not required) for max(m,n+1) to be a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional n-long array of row indices in [1, max(m,n)] to\n",
    "              implement subsampling; generated by seed if not specified,\n",
    "              but may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length m\n",
    "        Ay(y): computes A'.y (of length m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    if new_embedding:\n",
    "        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "    else:\n",
    "        w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (n,)\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        rng.shuffle(idxs)\n",
    "        ordering = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == m, \"x must be m long\"\n",
    "        y = np.zeros(w)\n",
    "        if new_embedding:\n",
    "            y[w-m:] = x.reshape(m)\n",
    "        else:\n",
    "            y[:m] = x.reshape(m)\n",
    "        fht(y)\n",
    "        return y[ordering]\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n, \"input must be n long\"\n",
    "        x = np.zeros(w)\n",
    "        x[ordering] = y.reshape(n)\n",
    "        fht(x)\n",
    "        if new_embedding:\n",
    "            return x[w-m:]\n",
    "        else:\n",
    "            return x[:m]\n",
    "\n",
    "    return Ax, Ay, ordering\n",
    "\n",
    "def block_sub_fht(n, m, l, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially\n",
    "    offering substantial speed improvements.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns per block\n",
    "    l: number of blocks\n",
    "\n",
    "    It is most efficient (though not required) when max(m,n+1) is a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to\n",
    "              implement subsampling; generated by seed if not specified, but\n",
    "              may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length l*m\n",
    "        Ay(y): computes A'.y (of length l*m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    assert l > 0, \"l must be positive\"\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (l, n)\n",
    "    else:\n",
    "        if new_embedding:\n",
    "            w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "        else:\n",
    "            w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        ordering = np.empty((l, n), dtype=np.uint32)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        for ll in range(l):\n",
    "            rng.shuffle(idxs)\n",
    "            ordering[ll] = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == l*m\n",
    "        out = np.zeros(n)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out += ax(x[ll*m:(ll+1)*m])\n",
    "        return out\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n\n",
    "        out = np.empty(l*m)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out[ll*m:(ll+1)*m] = ay(y)\n",
    "        return out\n",
    "\n",
    "    return Ax, Ay, ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Hadamard Transforms\n",
    "\n",
    "This code can all be found in `pyfht`, which uses a C extension to speed up the fht function. To make this notebook self contained, it's reproduced entirely in Python here, which will be quite slow!\n",
    "\n",
    "Skip to the next section if you're not interested in the specific transform implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfht import block_sub_fht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree encoder\n",
    "\n",
    "This function encodes the payloads corresponding to users into codewords from the specified tree code. \n",
    "\n",
    "Parity bits in section $i$ are generated based on the information sections $i$ is connected to\n",
    "\n",
    "Computations are done within the ring of integers modulo length of the section to enable FFT-based BP on the outer graph\n",
    "\n",
    "This function outputs the sparse representation of encoded messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx_message is a (Ka x w) matrix containing Ka messages along its rows\n",
    "# Ka is the number of active users\n",
    "# messageBlocks is an indicator vector of which blocks are information blocks\n",
    "# G is the generator matrix that defines the outer graph\n",
    "# L is the number of sections \n",
    "# vl is the length of each coded section\n",
    "\n",
    "def Tree_encode(tx_message, Ka, messageBlocks, G, L, vl):\n",
    "    encoded_tx_message = np.zeros((Ka,L), dtype=int)            # instantiate (Ka x L) matrix to hold encoded message\n",
    "    \n",
    "    # Iterate through L blocks\n",
    "    for i in range(0,L):\n",
    "        \n",
    "        # check to see if current block is an information block\n",
    "        if messageBlocks[i]:\n",
    "            \n",
    "            # if so, directly take bits from tx_message for information block\n",
    "            encoded_tx_message[:,i] = tx_message[:,np.sum(messageBlocks[:i])*vl:(np.sum(messageBlocks[:i])+1)*vl] \\\n",
    "                                      .dot(2**np.arange(vl)[::-1])\n",
    "            \n",
    "        # current block is a parity block.  Use equation (8)\n",
    "        else:\n",
    "            \n",
    "            # nonzero elements of G[i] indicate which information blocks the current parity block is connected to\n",
    "            indices = np.where(G[i])[0] \n",
    "            \n",
    "            # instantiate data structure to hold parity information - (Ka x 1) vector of all zeros\n",
    "            ParityInteger=np.zeros((Ka,1), dtype='int') \n",
    "            \n",
    "            # sum of j in Wl (8)\n",
    "            for j in indices:\n",
    "                temp_summand = encoded_tx_message[:,j].reshape(-1,1)      # get column vector of messages in information block j\n",
    "                ParityInteger = np.mod(ParityInteger+temp_summand,2**vl)  # perform addition over ring of integers mod 2^vl\n",
    "                \n",
    "            # store parity blocks inside encoded_tx_message\n",
    "            encoded_tx_message[:,i] = ParityInteger.reshape(-1)\n",
    "    \n",
    "    # return encoded message\n",
    "    return encoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts message indices into $L$-sparse vectors of length $L 2^{v_{l}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_tx_message_indices is a (Ka x L) matrix holding the encoded messages for Ka active users\n",
    "# L is the number of sections\n",
    "# vl is the length of a coded section\n",
    "# Ka is the number of active users\n",
    "\n",
    "def convert_indices_to_sparse(encoded_tx_message_indices, L, vl, Ka):\n",
    "    encoded_tx_message_sparse=np.zeros((L*2**vl,1), dtype=int)     # instantiate data structure for sparse coded messages\n",
    "    \n",
    "    # iterate over each block\n",
    "    for i in range(L):\n",
    "        A = encoded_tx_message_indices[:,i]                        # extract index representation of current block\n",
    "        B = A.reshape([-1,1])                                      # convert from row vector to column vector\n",
    "        np.add.at(encoded_tx_message_sparse, i*2**vl+B, 1)         # add sparse representation of block to encoded message\n",
    "        \n",
    "    # return sparse representation of encoded message\n",
    "    return encoded_tx_message_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the index representation corresponding to a SPARC-like vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_to_indices(cs_decoded_tx_message_sparse,L,J,listSize):\n",
    "    cs_decoded_tx_message = np.zeros((listSize,L),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = cs_decoded_tx_message_sparse[i*2**J:(i+1)*2**J]\n",
    "        idx = (A.reshape(2**J,)).argsort()[np.arange(2**J-listSize)]\n",
    "        B = np.setdiff1d(np.arange(2**J),idx)\n",
    "        cs_decoded_tx_message[:,i] = B \n",
    "\n",
    "    return cs_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information bits from retained paths in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_indices(Paths,cs_decoded_tx_message, L,J):\n",
    "    msg_bits = np.empty(shape=(0,0))\n",
    "    L1 = Paths.shape[0]\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        msg_bits = np.vstack((msg_bits,msg_bit)) if msg_bits.size else msg_bit           \n",
    "    return msg_bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Codebook\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A.\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\cdot B$ and $z^T\\cdot A$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparc_codebook(L, M, n,P):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1)/ np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1)/ np.sqrt(n) \n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP on outer graph\n",
    "\n",
    "This function computes the priors on the unknown sparse vector, given effective obervations of the (graph) neighboring sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = effective observation\n",
    "# G = matrix that defines the graphical structure of outer code\n",
    "# messageBlock = indicator vector of which blocks are information blocks\n",
    "# l = number of sections\n",
    "# ml = length of message section\n",
    "# p0 = uninformative prior\n",
    "# Ka = number of users\n",
    "# τ = standard deviation of noise\n",
    "# Phat = estimate of tx power\n",
    "# numBPiter = number of belief propagation iterations to perform\n",
    "\n",
    "# it appears that beta here refers to lambda in the paper...\n",
    "\n",
    "# Notation has been adapted to match https://arxiv.org/pdf/2010.04364.pdf\n",
    "def computePrior(s, G, messageBlocks, L, ml, p0, Ka, τ, Phat, numBPiter):\n",
    "    \n",
    "    # Initialize data structures\n",
    "    q = np.zeros(s.shape,dtype=float)              # vector of priors to be returned to AMP algorithm\n",
    "    p1 = p0*np.ones(s.shape,dtype=float)           # working estimate of priors to be used throughout BP algorithm\n",
    "    μ = np.ones((L, 3, 2**vl))                     # data structure for storing messages passed on factor graph\n",
    "    \n",
    "    '''\n",
    "    We will use a single data structure (μ) to hold all messages - both from information to parity nodes and parity to\n",
    "    information nodes.  There will be one row for every section.  The row represents the node that sent the message.\n",
    "    There are three columns - one for each of the nodes a node may be connected to.  At each row, column pair, a length \n",
    "    2**vl array is found representing the message sent from the node in the row position to the node in the column position. \n",
    "    \n",
    "    Note that this collapsed matrix (i.e, L x 3 instead of L x L) will result in some interesting indexing operations.  The\n",
    "    impetus behind using such a collapsed structure was the need to conserve memory. \n",
    "    '''\n",
    "\n",
    "    # Compute local (instrinsic) estimate vector - see equation (38) and Lemma 3 (eq (30))\n",
    "    temp_λ = (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2))) / (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-p1)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "    λ = temp_λ.reshape(L,-1)                       # (L, 2**vl) matrix\n",
    "    λ = λ/(np.sum(λ,axis=1).reshape(L,-1))         # Normalize the rows of λ so that λ is a valid PME\n",
    "    \n",
    "    \n",
    "    # Perform numBPiter iterations of the BP algorithm\n",
    "    for iter in range(numBPiter):\n",
    "        \n",
    "        \n",
    "        # Step 1: Compute messages μ(sl -> a) -- see equation (18)\n",
    "        for r in range(L):\n",
    "            if not messageBlocks[r]:               # skip parity blocks\n",
    "                continue\n",
    "            parityIdx = np.where(G[r])[0]          # determine which parity blocks the current info block is connected to \n",
    "            \n",
    "            # compute messages from current info block to each of the above-identified parity blocks\n",
    "            for c in range(len(parityIdx)):\n",
    "                μ[r, c] = np.ones(2**vl)           # reset message to all \"1\"s\n",
    "                μ[r, c] = μ[r, c] * λ[r,:]         # all messages get multiplied by λ\n",
    "                \n",
    "                # \\prod_{a \\in N(s_l) \\ a} in eq (18)\n",
    "                for d in range(len(parityIdx)):\n",
    "                    if c != d:                     # ensure that we skip the recipient of the message\n",
    "                        tmpIdx = np.where(G[parityIdx[d]])[0]\n",
    "                        μ[r, c] = μ[r, c] * μ[parityIdx[d], np.where(tmpIdx == r)[0][0]]\n",
    "                \n",
    "                μ[r, c] = μ[r, c]/np.sum(μ[r, c])  # normalize answer\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Step 2: Compute messages μ(ap -> s) -- see equation (17) and section \"Design Considerations for Fast Execution\"\n",
    "        for r in range(L):\n",
    "            if messageBlocks[r]:                    # skip info blocks\n",
    "                continue\n",
    "            infoIdx = np.where(G[r])[0]             # find all info blocks are connected to the current parity block\n",
    "            μ[r, :] = np.ones(2**vl)                # reset message\n",
    "            \n",
    "            # compute messages from current parity block to each of the above-identified info blocks\n",
    "            for j in range(len(infoIdx)):\n",
    "                \n",
    "                # sj \\in N(ap) \\ sl\n",
    "                for k in range(len(infoIdx)):\n",
    "                    if k != j:                      # setminus the recipient of the message\n",
    "                        tmpIdx = np.where(G[infoIdx[k]])[0]\n",
    "                        μ[r, k] = μ[r, k] * np.fft.fft(μ[infoIdx[k], np.where(tmpIdx == r)[0][0]])\n",
    "                \n",
    "                μ[r, j] = np.fft.ifft(μ[r, j]).real                        \n",
    "                μ[r, j] = μ[r, j]/np.linalg.norm(μ[r, j], ord=0)  # normalize answer\n",
    "                \n",
    "        \n",
    "        # Ensure that all values in μ are valid\n",
    "        np.nan_to_num(μ)\n",
    "    \n",
    "    \n",
    "    # After performing maxBPIter rounds of belief propagation, compute proper belief vector for AMP denoiser\n",
    "    # see equation (39)\n",
    "    for i in range(L):\n",
    "        \n",
    "        # get indices of blocks connected to current block\n",
    "        blockIdx = np.where(G[i])[0]\n",
    "        \n",
    "        # initialize required data structure\n",
    "        temp_p1 = np.ones(2**vl)\n",
    "            \n",
    "        # compute μ_{sl}\n",
    "        for j in blockIdx:\n",
    "            tempIdx = np.where(G[j])[0]\n",
    "            temp_p1 = temp_p1 * μ[j, np.where(tempIdx == i)[0][0]]\n",
    "\n",
    "        # use equation 39 to normalize properly\n",
    "        p1[i*ml:(i+1)*ml] = (temp_p1/np.sum(temp_p1)).reshape(-1, 1)\n",
    "        p1[i*ml:(i+1)*ml] = 1-(1-p1[i*ml:(i+1)*ml])**Ka\n",
    "            \n",
    "    \n",
    "    # ensure that no value of p1 exceeds 1\n",
    "    q = np.minimum(p1, 1)\n",
    "    \n",
    "    # return q\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but note we use `longdouble` types because the expentials are often too big to fit into a normal `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp(y, σ_n, P, L, ml, numAmpIter, Ab, Az, p0, Ka, G, messageBlocks, BPonOuterGraph, numBPiter):\n",
    "\n",
    "    N = y.size                                  # dimension of vector y\n",
    "    β = np.zeros((L*ml, 1))                     # initialize vector to store beta coefficients\n",
    "    z = y.copy()                                # create deep copy of y for AMP algorithm\n",
    "    Phat = N*P/L                                # Estimate of transmit power\n",
    "    τ_evolution = np.zeros((numAmpIter,1))      # Store the values of τ corresponding to each iteration\n",
    "    \n",
    "    # perform T iterations of AMP\n",
    "    for t in range(numAmpIter):\n",
    "        \n",
    "        τ = np.sqrt(np.sum(z**2)/N)                                         # Compute τ online using the residual\n",
    "        s = (np.sqrt(Phat)*β + Az(z)).astype(np.longdouble)                 # effective observation\n",
    "        \n",
    "        # select proper prior\n",
    "        if BPonOuterGraph==0:\n",
    "            q = p0                                                          # Use the uninformative prior p0 for Giuseppe's scheme\n",
    "        else:\n",
    "            q = computePrior(s,G,messageBlocks,L,ml,p0,Ka,τ,Phat,numBPiter) # Compute the prior through BP on outer graph\n",
    "            \n",
    "        # denoiser\n",
    "        β = (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ \\\n",
    "            (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + \\\n",
    "            (1-q)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "        \n",
    "        # residual\n",
    "        z = y - np.sqrt(Phat)*Ab(β) + (z/(N*τ**2)) * (Phat*np.sum(β) - Phat*np.sum(β**2))\n",
    "        \n",
    "        # store tau value for debugging purposes\n",
    "        τ_evolution[t] = τ\n",
    "\n",
    "    # return β and τ_evolution\n",
    "    return β, τ_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree decoder\n",
    "\n",
    "This function implements the tree deocoder for a specific graph corresponding to the outer tree code\n",
    "\n",
    "It is currently hard-coded for a specfic architecture\n",
    "\n",
    "The architecture is based on a tri-adic design and can be found in the simulation results section of https://arxiv.org/pdf/2001.03705.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_decoder(cs_decoded_tx_message,G,L,J,B,listSize):\n",
    "    \n",
    "    tree_decoded_tx_message = np.empty(shape=(0,0))\n",
    "    \n",
    "    Paths012 = merge_paths(cs_decoded_tx_message[:,0:3])\n",
    "    \n",
    "    Paths345 = merge_paths(cs_decoded_tx_message[:,3:6])\n",
    "    \n",
    "    Paths678 = merge_paths(cs_decoded_tx_message[:,6:9])\n",
    "    \n",
    "    Paths91011 = merge_paths(cs_decoded_tx_message[:,9:12])\n",
    "    \n",
    "    Paths01267812 = merge_pathslevel2(Paths012,Paths678,cs_decoded_tx_message[:,[0,6,12]])\n",
    "    \n",
    "    Paths3459101113 = merge_pathslevel2(Paths345,Paths91011,cs_decoded_tx_message[:,[3,9,13]])\n",
    "    \n",
    "    Paths01267812345910111314 = merge_all_paths0(Paths01267812,Paths3459101113,cs_decoded_tx_message[:,[1,4,10,14]])\n",
    "    \n",
    "    Paths = merge_all_paths_final(Paths01267812345910111314,cs_decoded_tx_message[:,[7,10,15]])\n",
    "    \n",
    "    \n",
    "   \n",
    "    return Paths\n",
    "\n",
    "def merge_paths(A):\n",
    "    listSize = A.shape[0]\n",
    "    B = np.array([np.mod(A[:,0] + a,2**16) for a in A[:,1]]).flatten()\n",
    "     \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,listSize).reshape(-1,1),np.floor(I/listSize).reshape(-1,1)]).astype(int)\n",
    "            Paths = np.vstack((Paths,np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)]))) if Paths.size else np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)])\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_pathslevel2(Paths012,Paths678,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths0 = Paths012[:,0]\n",
    "    Paths6 = Paths678[:,0]\n",
    "    B = np.array([np.mod(A[Paths0,0] + a,2**16) for a in A[Paths6,1]]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths0.shape[0]).reshape(-1,1),np.floor(I/Paths0.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths012[I1[:,0]].reshape(-1,3),Paths678[I1[:,1]].reshape(-1,3),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "               \n",
    "    return Paths\n",
    "\n",
    "\n",
    "def merge_all_paths0(Paths01267812,Paths3459101113,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths1 = Paths01267812[:,1]\n",
    "    Paths4 = Paths3459101113[:,1]\n",
    "    Paths10 = Paths3459101113[:,4]\n",
    "    Aa = np.mod(A[Paths4,1]+A[Paths10,2],2**16)\n",
    "    B = np.array([np.mod(A[Paths1,0] + a,2**16) for a in Aa]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,3])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths1.shape[0]).reshape(-1,1),np.floor(I/Paths1.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths01267812[I1[:,0]].reshape(-1,7),Paths3459101113[I1[:,1]].reshape(-1,7),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_all_paths_final(Paths01267812345910111314,A):\n",
    "    \n",
    "    listSize = A.shape[0]\n",
    "    Paths7 = Paths01267812345910111314[:,4]\n",
    "    Paths10 = Paths01267812345910111314[:,11]\n",
    "    B = np.mod(A[Paths7,0] + A[Paths10,1] ,2**16)\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            PPaths = np.hstack((Paths01267812345910111314[I].reshape(-1,15),np.repeat(i,I.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    return Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tree decoder outputs more than $K$ valid paths, retain $K-\\delta$ of them based on their LLRs\n",
    "\n",
    "$\\delta$ is currently set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_topKminusdelta_paths(Paths, cs_decoded_tx_message, β, J,K,delta):\n",
    "    \n",
    "    L1 = Paths.shape[0]\n",
    "    LogL = np.zeros((L1,1))\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,j*(2**J)+cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else j*(2**J)+cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        LogL[i] = np.sum(np.log(β[msg_bit])) \n",
    "    Indices =  LogL.reshape(1,-1).argsort()[0,-(K-delta):]\n",
    "    Paths = Paths[Indices,:].reshape(((K-delta),-1))\n",
    "    \n",
    "    return Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up multiple access scenario\n",
    "# Notation has been chosen to match https://arxiv.org/pdf/2010.04364.pdf\n",
    "Ka = 25                 # Number of active users\n",
    "w = 128                 # Payload size of each active user (per user message length)\n",
    "L = 16                  # Number of sections/sub-blocks\n",
    "N = 38400               # Total number of channel uses (real d.o.f)\n",
    "vl = 16                 # Length of each coded sub-block\n",
    "ml = 2**vl              # Length of each section of m\n",
    "\n",
    "# Simulation Parameters\n",
    "numAmpIter = 6          # Number of AMP iterations\n",
    "listSize = 4 * Ka       # List size retained for each section after AMP converges\n",
    "BPonOuterGraph = 1      # Indicates whether to perform BP on outer code.  If 0, AMP uses Giuseppe's uninformative prior\n",
    "numBPiter = 2;          # Number of BP iterations on outer code\n",
    "EbNodB = 2.4            # Energy per bit in decibels\n",
    "maxSims=10              # Number of Simulations to Run\n",
    "\n",
    "# Set Up Outer Tree Code\n",
    "messageBlocks = np.array([1,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0]).astype(int)    # Indicates the indices of information blocks\n",
    "G = np.zeros((L,L)).astype(int)        # Define outer code factor graph via matrix G\n",
    "G[0,[2,12]]=1\n",
    "G[1,[2,14]]=1\n",
    "G[2,[0,1]]=1\n",
    "G[3,[5,13]]=1\n",
    "G[4,[5,14]]=1\n",
    "G[5,[3,4]]=1\n",
    "G[6,[8,12]]=1\n",
    "G[7,[8,15]]=1\n",
    "G[8,[6,7]]=1\n",
    "G[9,[11,13]]=1\n",
    "G[10,[11,14,15]]=1\n",
    "G[11,[9,10]]=1\n",
    "G[12,[0,6]]=1\n",
    "G[13,[3,9]]=1\n",
    "G[14,[1,4,10]]=1\n",
    "G[15,[7,10]]=1\n",
    "\n",
    "print(G)\n",
    "\n",
    "# Prepare Simulation\n",
    "p0 = 1-(1-1/ml)**Ka     # Giuseppe's uninformative prior\n",
    "EbNo = 10**(EbNodB/10)  # Eb/No in linear scale\n",
    "P = 2*w*EbNo/N          # transmit power\n",
    "σ_n = 1                 # Noise standard deviation\n",
    "Phat = N*P/L            # Power estimate\n",
    "msgDetected=0           # Track number of detected messages.  Used for error computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CCS-AMP maxSims times\n",
    "for sims in range(maxSims):\n",
    "    \n",
    "    # Seed RNG for consistency. Remove after testing.  The number 17 was chosen arbitrarily\n",
    "    np.random.seed(17)\n",
    "    \n",
    "    # Generate messages for Ka active users\n",
    "    tx_message = np.random.randint(low=2, size=(Ka, w))\n",
    "    \n",
    "    # Outer-encode the message sequences\n",
    "    encoded_tx_message_indices = Tree_encode(tx_message, Ka, messageBlocks, G, L, vl)\n",
    "    \n",
    "    # Convert indices to sparse representation\n",
    "    β_0 = convert_indices_to_sparse(encoded_tx_message_indices, L, vl, Ka)\n",
    "    \n",
    "    # Generate the binned SPARC codebook\n",
    "    Ab, Az = sparc_codebook(L, ml, N, P)\n",
    "    \n",
    "    # Generate our transmitted signal X\n",
    "    x = np.sqrt(Phat)*Ab(β_0)\n",
    "    \n",
    "    # Generate random channel noise and thus also received signal y\n",
    "    z = np.random.randn(N, 1) * σ_n\n",
    "    y = (x + z).reshape(-1, 1)\n",
    "    \n",
    "    # Run AMP decoding\n",
    "    β, τ_evolution = amp(y, σ_n, P, L, ml, numAmpIter, Ab, Az,p0,Ka,G,messageBlocks,BPonOuterGraph,numBPiter)\n",
    "    \n",
    "    # FIXME: Remove the following line of code\n",
    "    raise Exception('Stopping...')\n",
    "    \n",
    "    # Convert decoded sparse vector into vector of indices  \n",
    "    cs_decoded_tx_message = convert_sparse_to_indices(β,L,vl,listSize)\n",
    "\n",
    "    # Tree decoder to decode individual messages from lists output by AMP\n",
    "    Paths = Tree_decoder(cs_decoded_tx_message,G,L,vl,w,listSize)\n",
    "    \n",
    "    # Re-align paths to the correct order\n",
    "    perm = np.argsort(np.array([0,1,2,6,7,8,12,3,4,5,9,10,11,13,14,15]))\n",
    "    Paths = Paths[:,perm]\n",
    "    \n",
    "    # If tree deocder outputs more than K valid paths, retain only K of them\n",
    "    if Paths.shape[0] > Ka:\n",
    "        Paths = pick_topKminusdelta_paths(Paths, cs_decoded_tx_message, β, vl, Ka, 0)\n",
    "\n",
    "    # Extract the message indices from valid paths in the tree    \n",
    "    Tree_decoded_indices = extract_msg_indices(Paths, cs_decoded_tx_message, L, vl)\n",
    "\n",
    "    # Calculation of per-user prob err\n",
    "    for i in range(Ka):\n",
    "        msgDetected = msgDetected + np.equal(encoded_tx_message_indices[i,:],Tree_decoded_indices).all(axis=1).any()\n",
    "\n",
    "    \n",
    "errorRate= (Ka*maxSims - msgDetected)/(Ka*maxSims)\n",
    "\n",
    "print(\"Per user probability of error = \", errorRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
