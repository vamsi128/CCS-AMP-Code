{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS-AMP for unsourced multiple access\n",
    "\n",
    "This notebook contains CCS-AMP encoder/decoder for unsourced multiple access using Hadamard design matrices.\n",
    "\n",
    "The code is based on the following articles:\n",
    "\n",
    "A coded compressed sensing scheme for uncoordinated multiple access, available @ https://arxiv.org/pdf/1809.04745.pdf\n",
    "\n",
    "SPARCs for Unsourced Random Access, available @ https://arxiv.org/abs/1901.06234\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "We just need a modern numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fht(u):\n",
    "    \"\"\"\n",
    "    Perform fast Hadamard transform of u, in-place.\n",
    "    Note len(u) must be a power of two.\n",
    "    \"\"\"\n",
    "    N = len(u)\n",
    "    i = N>>1\n",
    "    while i:\n",
    "        for j in range(N):\n",
    "            if (i&j) == 0:\n",
    "                temp = u[j]\n",
    "                u[j] += u[i|j]\n",
    "                u[i|j] = temp - u[i|j]\n",
    "        i>>= 1\n",
    "\n",
    "def sub_fht(n, m, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    Returns functions to compute the sub-sampled Walsh-Hadamard transform,\n",
    "    i.e., operating with a wide rectangular matrix of random +/-1 entries.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns\n",
    "\n",
    "    It is most efficient (but not required) for max(m,n+1) to be a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional n-long array of row indices in [1, max(m,n)] to\n",
    "              implement subsampling; generated by seed if not specified,\n",
    "              but may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length m\n",
    "        Ay(y): computes A'.y (of length m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    if new_embedding:\n",
    "        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "    else:\n",
    "        w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (n,)\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        rng.shuffle(idxs)\n",
    "        ordering = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == m, \"x must be m long\"\n",
    "        y = np.zeros(w)\n",
    "        if new_embedding:\n",
    "            y[w-m:] = x.reshape(m)\n",
    "        else:\n",
    "            y[:m] = x.reshape(m)\n",
    "        fht(y)\n",
    "        return y[ordering]\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n, \"input must be n long\"\n",
    "        x = np.zeros(w)\n",
    "        x[ordering] = y.reshape(n)\n",
    "        fht(x)\n",
    "        if new_embedding:\n",
    "            return x[w-m:]\n",
    "        else:\n",
    "            return x[:m]\n",
    "\n",
    "    return Ax, Ay, ordering\n",
    "\n",
    "def block_sub_fht(n, m, l, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially\n",
    "    offering substantial speed improvements.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns per block\n",
    "    l: number of blocks\n",
    "\n",
    "    It is most efficient (though not required) when max(m,n+1) is a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to\n",
    "              implement subsampling; generated by seed if not specified, but\n",
    "              may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length l*m\n",
    "        Ay(y): computes A'.y (of length l*m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    assert l > 0, \"l must be positive\"\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (l, n)\n",
    "    else:\n",
    "        if new_embedding:\n",
    "            w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "        else:\n",
    "            w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        ordering = np.empty((l, n), dtype=np.uint32)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        for ll in range(l):\n",
    "            rng.shuffle(idxs)\n",
    "            ordering[ll] = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == l*m\n",
    "        out = np.zeros(n)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out += ax(x[ll*m:(ll+1)*m])\n",
    "        return out\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n\n",
    "        out = np.empty(l*m)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out[ll*m:(ll+1)*m] = ay(y)\n",
    "        return out\n",
    "\n",
    "    return Ax, Ay, ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Hadamard Transforms\n",
    "\n",
    "This code can all be found in `pyfht`, which uses a C extension to speed up the fht function. To make this notebook self contained, it's reproduced entirely in Python here, which will be quite slow!\n",
    "\n",
    "Skip to the next section if you're not interested in the specific transform implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have pyfht installed (via pip etc) you can import it here\n",
    "# to use its C-accelerated transform\n",
    "#from pyfht import block_sub_fht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parity Generator Matrix\n",
    "This function builds the binary parity generator matrix for the outer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parity_matrix(L,messageLengthVector,parityLengthVector):\n",
    "    # Generate a full matrix, use only the portion needed for tree code\n",
    "    G = []\n",
    "    for i in range(1,L):\n",
    "        Gp = np.random.randint(2,size=(np.sum(messageLengthVector[0:i]),parityLengthVector[i])).tolist()\n",
    "        G.append(Gp)\n",
    "    return np.asarray(G)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree encoder\n",
    "\n",
    "This function encodes the payloads corresponding to users into codewords from the specified tree code. \n",
    "\n",
    "Parity bits in section $i$ are generated based on the message bits from all the previous sections $[0:i-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_encode(tx_message,K,G,L,J,P,Ml,messageLengthVector,parityLengthVector):\n",
    "    encoded_tx_message = np.zeros((K,Ml+P),dtype=int)\n",
    "    encoded_tx_message[:,0:messageLengthVector[0]] = tx_message[:,0:messageLengthVector[0]]\n",
    "    for i in range(1,L):\n",
    "        ParityInteger=np.zeros((K,1),dtype='int')\n",
    "        G1=G[i-1]\n",
    "        for j in range(1,i+1):\n",
    "            ParityBinary = np.mod(np.matmul(tx_message[:,np.sum(messageLengthVector[0:j-1]):np.sum(messageLengthVector[0:j])],\n",
    "                                G1[np.sum(messageLengthVector[0:j-1]):np.sum(messageLengthVector[0:j])]),2)\n",
    "            # Convert into decimal equivalent\\n\",\n",
    "            ParityInteger1 = ParityBinary.dot(2**np.arange(ParityBinary.shape[1])[::-1]).reshape([K,1])\n",
    "            ParityInteger = np.mod(ParityInteger+ParityInteger1,2**parityLengthVector[i])\n",
    "        # Convert integer parity back into bit    \\n\",\n",
    "        Parity = np.array([list(np.binary_repr(int(x),parityLengthVector[i])) for x in ParityInteger], dtype=int)\n",
    "        encoded_tx_message[:,i*J:i*J+messageLengthVector[i]] = tx_message[:,np.sum(messageLengthVector[0:i]):np.sum(messageLengthVector[0:i+1])]\n",
    "        # Embed Parity check bits\\n\",\n",
    "        encoded_tx_message[:,i*J+messageLengthVector[i]:(i+1)*J] = Parity\n",
    "    \n",
    "    return encoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts message sequence into $L$-sparse vectors of length $L 2^J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bits_to_sparse(encoded_tx_message,L,J,K):\n",
    "    encoded_tx_message_sparse=np.zeros((L*2**J,1),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = encoded_tx_message[:,i*J:(i+1)*J]\n",
    "        B = A.dot(2**np.arange(J)[::-1]).reshape([K,1])\n",
    "        np.add.at(encoded_tx_message_sparse, i*2**J+B, 1)        \n",
    "    return encoded_tx_message_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reurns the bit representation corresponding to a SPARC-like vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_to_bits(cs_decoded_tx_message_sparse,L,J,listSize):\n",
    "    cs_decoded_tx_message = np.zeros((listSize,L*J),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = cs_decoded_tx_message_sparse[i*2**J:(i+1)*2**J]\n",
    "        idx = (A.reshape(2**J,)).argsort()[np.arange(2**J-listSize)]\n",
    "        B = np.setdiff1d(np.arange(2**J),idx)\n",
    "        C = np.empty(shape=(0,0),dtype=int)\n",
    "        for j in B:\n",
    "            C = np.hstack((C,np.array([j],dtype=int))) if C.size else np.array([j],dtype=int)\n",
    "        cs_decoded_tx_message[:,i*J:(i+1)*J]=np.array([list(np.binary_repr(int(x),J)) for x in C], dtype=int)    \n",
    "    return cs_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information bits from retained paths in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_bits(Paths,cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector):\n",
    "    msg_bits = np.empty(shape=(0,0))\n",
    "    L1 = Paths.shape[0]\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,cs_decoded_tx_message[path[0,j],J*j:J*j+messageLengthVector[j]].reshape(1,-1))) if msg_bit.size else cs_decoded_tx_message[path[0,j],J*(j):J*(j)+messageLengthVector[j]]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        msg_bits = np.vstack((msg_bits,msg_bit)) if msg_bits.size else msg_bit           \n",
    "    return msg_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the possible parity check bits for the next section, given a path in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_permissible_parity(Path,cs_decoded_tx_message,G1,L,J,parityLengthVector,messageLengthvector):\n",
    "    msg_bits = extract_msg_bits(Path,cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "    Lpath = Path.shape[1]\n",
    "    Parity_computed_integer = 0\n",
    "    for i in range(Lpath):\n",
    "        ParityBinary = np.mod(np.matmul(msg_bits[:,np.sum(messageLengthVector[0:i]):np.sum(messageLengthVector[0:i+1])],\n",
    "                            G1[np.sum(messageLengthVector[0:i]):np.sum(messageLengthVector[0:i+1])]),2)\n",
    "        ParityBinary=ParityBinary.reshape(1,-1)\n",
    "        # Convert into decimal equivalent\\n\",\n",
    "        ParityInteger1 = ParityBinary.dot(2**np.arange(ParityBinary.shape[1])[::-1])\n",
    "        Parity_computed_integer = np.mod(Parity_computed_integer+ParityInteger1,2**parityLengthVector[Lpath])        \n",
    "         \n",
    "    Parity_computed = np.array([list(np.binary_repr(int(x),parityLengthVector[Lpath])) for x in Parity_computed_integer], dtype=int)\n",
    "    return Parity_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the parity check constraints for a section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_check(Parity_computed,Path,k,cs_decoded_tx_message,L,J,parityLengthVector,messageLengthvector):\n",
    "    index=0\n",
    "    Lpath = Path.shape[1]\n",
    "    Parity = cs_decoded_tx_message[k,Lpath*J+messageLengthvector[Lpath]:(Lpath+1)*J]\n",
    "    if (np.sum(np.absolute(Parity_computed-Parity)) == 0):\n",
    "        index = 1\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if multiple paths output by the tree decoder are all comprosed of same information bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_identical_msgs(Paths, cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector):   \n",
    "    msg_bits = extract_msg_bits(Paths,cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "    flag = (msg_bits == msg_bits[0]).all()    \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree decoder\n",
    "\n",
    "This function implements the tree deocoder proposed in the paper \"A coded compressed sensing scheme for uncoordinated multiple access\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_decoder(cs_decoded_tx_message,G,L,J,B,parityLengthVector,messageLengthvector,listSize):\n",
    "    tree_decoded_tx_message = np.empty(shape=(0,0))\n",
    "    for i in range(listSize):\n",
    "        Paths = np.array([[i]])\n",
    "        for l in range(1,L):\n",
    "            # Grab the parity generator matrix corresponding to this section\n",
    "            G1 = G[l-1]\n",
    "            new=np.empty( shape=(0,0))\n",
    "            for j in range(Paths.shape[0]):\n",
    "                Path=Paths[j].reshape(1,-1)\n",
    "                # Compute the permissible parity check bits for the section\n",
    "                Parity_computed = compute_permissible_parity(Path,cs_decoded_tx_message,G1,L,J,parityLengthVector,messageLengthvector)\n",
    "                for k in range(listSize):\n",
    "                    # Verify parity constraints for the children of surviving path\n",
    "                    index = parity_check(Parity_computed,Path,k,cs_decoded_tx_message,L,J,parityLengthVector,messageLengthvector)\n",
    "                    # If parity constraints are satisfied, update the path\n",
    "                    if index:\n",
    "                        new = np.vstack((new,np.hstack((Path.reshape(1,-1),np.array([[k]]))))) if new.size else np.hstack((Path.reshape(1,-1),np.array([[k]])))\n",
    "            Paths = new \n",
    "        if Paths.shape[0] >= 2:\n",
    "            # If tree decoder outputs multiple paths for a root node, select the first one \n",
    "            flag = check_if_identical_msgs(Paths, cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "            if flag:\n",
    "                tree_decoded_tx_message = np.vstack((tree_decoded_tx_message,extract_msg_bits(Paths[0].reshape(1,-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector))) if tree_decoded_tx_message.size else extract_msg_bits(Paths[0].reshape(1,-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "            else:\n",
    "                tree_decoded_tx_message = np.vstack((tree_decoded_tx_message,extract_msg_bits(Paths.reshape(Paths.shape[0],-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector))) if tree_decoded_tx_message.size else extract_msg_bits(Paths.reshape(Paths.shape[0],-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "        elif Paths.shape[0] == 1:\n",
    "            tree_decoded_tx_message = np.vstack((tree_decoded_tx_message,extract_msg_bits(Paths.reshape(1,-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector))) if tree_decoded_tx_message.size else extract_msg_bits(Paths.reshape(1,-1),cs_decoded_tx_message, L,J,parityLengthVector,messageLengthvector)\n",
    "    return tree_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Codebook\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A.\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\cdot B$ and $z^T\\cdot A$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparc_codebook(L, M, n):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1)/ np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1)/ np.sqrt(n)\n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but note we use `longdouble` types because the expentials are often too big to fit into a normal `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp(y, σ_n, P, L, M, T, Ab, Az, p0, K):\n",
    "    n = y.size\n",
    "    β = np.zeros((L*M, 1))\n",
    "    z = y\n",
    "    Phat = n*P/L\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        τ = np.sqrt(np.sum(z**2)/n)\n",
    "        # effective observation\n",
    "        s = (np.sqrt(Phat)*β + Az(z)).astype(np.longdouble) \n",
    "        # denoiser\n",
    "        β = (p0*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ (p0*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-p0)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "        # residual\n",
    "        z = y - np.sqrt(Phat)*Ab(β) + (z/(n*τ**2)) * (Phat*np.sum(β) - Phat*np.sum(β**2))\n",
    "        #print(t,τ)\n",
    "\n",
    "    return β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04\n"
     ]
    }
   ],
   "source": [
    "K=25 # Number of active users\n",
    "B=100 # Payload size of each active user\n",
    "L=16 # Number of sections/sub-blocks\n",
    "n=30000 # Total number of channel uses (real d.o.f)\n",
    "T=10 # Number of AMP iterations\n",
    "listSize = 100  # List size output by the tree decoder for each section\n",
    "parityLengthVector = np.array([0,7,8,8,9,9,9,9,9,9,9,9,9,9,13,14],dtype=int) # Parity bits distribution\n",
    "J=((B+np.sum(parityLengthVector))/L).astype(int) # Length of each coded sub-block\n",
    "M=2**J # Length of each section\n",
    "messageLengthVector = np.subtract(J*np.ones(L, dtype = 'int'), parityLengthVector).astype(int)\n",
    "Pa = np.sum(parityLengthVector) # Total number of parity check bits\n",
    "Ml = np.sum(messageLengthVector) # Total number of information bits\n",
    "G = generate_parity_matrix(L,messageLengthVector,parityLengthVector)\n",
    "EbNodB = 4 \n",
    "p0 = 1-(1-1/M)**K\n",
    "maxSim=2 # number of simulations\n",
    "msgDetected=0\n",
    "\n",
    "# EbN0 in linear scale\n",
    "EbNo = 10**(EbNodB/10)\n",
    "P = 2*B*EbNo/n\n",
    "σ_n = 1\n",
    "# We assume equal power allocation for all the sections. Code has to be modified a little to accomodate different power allocations\n",
    "Phat = n*P/L\n",
    "    \n",
    "\n",
    "for s in range(maxSim):\n",
    "    \n",
    "\n",
    "    #G = generate_parity_matrix(L,messageLengthVector,parityLengthVector)\n",
    "    \n",
    "    # Generate active users message sequences\n",
    "    tx_message = np.random.randint(2, size=(K,B))\n",
    "    \n",
    "    # Outer-encode the message sequences\n",
    "    encoded_tx_message = Tree_encode(tx_message,K,G,L,J,Pa,Ml,messageLengthVector,parityLengthVector)\n",
    "    \n",
    "    # Convert bits to sparse representation\n",
    "    β_0 = convert_bits_to_sparse(encoded_tx_message,L,J,K)\n",
    "    \n",
    "    # Generate the binned SPARC codebook\n",
    "    Ab, Az = sparc_codebook(L, M, n)\n",
    "    \n",
    "    x = np.sqrt(Phat)*Ab(β_0)\n",
    "    \n",
    "    # Generate random channel noise and thus also received signal y\n",
    "    z = np.random.randn(n, 1) * σ_n\n",
    "    y = (x + z).reshape(-1, 1)\n",
    "\n",
    "    # Run AMP decoding\n",
    "    β = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K).reshape(-1)\n",
    "    \n",
    "    \n",
    "    # Convert decoded beta back to a message   \n",
    "    cs_decoded_tx_message = convert_sparse_to_bits(β,L,J,listSize)\n",
    "    \n",
    "    # Tree decoder to decode individual messages from lists output by AMP\n",
    "    tree_decoded_tx_message = Tree_decoder(cs_decoded_tx_message,G,L,J,B,parityLengthVector,messageLengthVector,listSize)\n",
    "    \n",
    "    # If tree deocder outputs more than K valid paths, retain only K of them\n",
    "    if tree_decoded_tx_message.shape[0]>K:\n",
    "        tree_decoded_tx_message = tree_decoded_tx_message[np.arange(K)]\n",
    "    \n",
    "    for i in range(tx_message.shape[0]):\n",
    "        msgDetected = msgDetected + np.equal(tx_message[i,:],tree_decoded_tx_message).all(axis=1).any()\n",
    "        \n",
    "errorRate= (K*maxSim - msgDetected)/(K*maxSim)\n",
    "\n",
    "print(errorRate)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
