{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for CCS-AMP Implementations\n",
    "\n",
    "This file seeks to test my implementation against the implementation by Vamsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import FactorGraphGeneration as FGG\n",
    "\n",
    "OuterCode1 = FGG.Graph8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Hadamard Transforms\n",
    "\n",
    "The ```PyFHT_local``` code can all be found in `pyfht`, which uses a C extension to speed up the fht function.\n",
    "Only one import suffices, with the latter being much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyFHT_local\n",
    "from pyfht import block_sub_fht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree encoder\n",
    "\n",
    "This function encodes the payloads corresponding to users into codewords from the specified tree code. \n",
    "\n",
    "Parity bits in section $i$ are generated based on the information sections $i$ is connected to\n",
    "\n",
    "Computations are done within the ring of integers modulo length of the section to enable FFT-based BP on the outer graph\n",
    "\n",
    "This function outputs the sparse representation of encoded messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_encode(tx_message,K,messageBlocks,G,L,J):\n",
    "    encoded_tx_message = np.zeros((K,L),dtype=int)\n",
    "    \n",
    "    encoded_tx_message[:,0] = tx_message[:,0:J].dot(2**np.arange(J)[::-1])\n",
    "    for i in range(1,L):\n",
    "        if messageBlocks[i]:\n",
    "            # copy the message if i is an information section\n",
    "            encoded_tx_message[:,i] = tx_message[:,np.sum(messageBlocks[:i])*J:(np.sum(messageBlocks[:i])+1)*J].dot(2**np.arange(J)[::-1])\n",
    "        else:\n",
    "            # compute the parity if i is a parity section\n",
    "            indices = np.where(G[i])[0]\n",
    "            ParityInteger=np.zeros((K,1),dtype='int')\n",
    "            for j in indices:\n",
    "                ParityInteger = ParityInteger + encoded_tx_message[:,j].reshape(-1,1)\n",
    "            encoded_tx_message[:,i] = np.mod(ParityInteger,2**J).reshape(-1)\n",
    "    \n",
    "    return encoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts message indices into $L$-sparse vectors of length $L 2^J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices_to_sparse(encoded_tx_message_indices,L,J,K):\n",
    "    aggregate_state_s_sparse=np.zeros((L*2**J,1),dtype=int)\n",
    "    for i in range(L):\n",
    "        section_indices_vectorized_rows = encoded_tx_message_indices[:,i]\n",
    "        section_indices_vectorized_cols = section_indices_vectorized_rows.reshape([-1,1])\n",
    "        np.add.at(aggregate_state_s_sparse, (i*2**J)+section_indices_vectorized_cols, 1)\n",
    "\n",
    "    return aggregate_state_s_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the index representation corresponding to a SPARC-like vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_to_indices(cs_decoded_tx_message_sparse,L,J,listSize):\n",
    "    cs_decoded_tx_message = np.zeros((listSize,L),dtype=int)\n",
    "    for i in range(L):\n",
    "        aggregate_section_sHat_sparse = cs_decoded_tx_message_sparse[i*2**J:(i+1)*2**J]\n",
    "        indices_low_values = (aggregate_section_sHat_sparse.reshape(2**J,)).argsort()[np.arange(2**J-listSize)]\n",
    "        indices_high_values = np.setdiff1d(np.arange(2**J),indices_low_values)\n",
    "        cs_decoded_tx_message[:,i] = indices_high_values\n",
    "\n",
    "    return cs_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information bits from retained paths in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_indices(Paths,cs_decoded_tx_message, L,J):\n",
    "    msg_bits = np.empty(shape=(0,0))\n",
    "    L1 = Paths.shape[0]\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        msg_bits = np.vstack((msg_bits,msg_bit)) if msg_bits.size else msg_bit\n",
    "\n",
    "    return msg_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Codebook\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A.\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\cdot B$ and $z^T\\cdot A$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparc_codebook(L, M, n,P):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, seed=None, ordering=None) # seed must be explicit\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1)/ np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1)/ np.sqrt(n) \n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Approximation\n",
    "\n",
    "This function outputs the closest approximation to the input vector given that its L1 norm is 1 and no entry is greater than 1/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximateVector(x, K):    \n",
    "\n",
    "    # normalize initial value of x\n",
    "    xOrig = x / np.linalg.norm(x, ord=1)\n",
    "    \n",
    "    # create vector to hold best approximation of x\n",
    "    xHt = xOrig.copy()\n",
    "    u = np.zeros(len(xHt))\n",
    "    \n",
    "    # run approximation algorithm\n",
    "    while np.amax(xHt) > (1/K):\n",
    "        minIndices = np.argmin([(1/K)*np.ones(xHt.shape), xHt], axis=0)\n",
    "        xHt = np.min([(1/K)*np.ones(xHt.shape), xHt], axis=0)\n",
    "        \n",
    "        deficit = 1 - np.linalg.norm(xHt, ord=1)\n",
    "        \n",
    "        if deficit > 0:\n",
    "            mIxHtNorm = np.linalg.norm((xHt*minIndices), ord=1)\n",
    "            scaleFactor = (deficit + mIxHtNorm) / mIxHtNorm\n",
    "            xHt = scaleFactor*(minIndices*xHt) + (1/K)*(np.ones(xHt.shape) - minIndices)\n",
    "\n",
    "    # return admissible approximation of x\n",
    "    return xHt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Mean Estimator (PME)\n",
    "\n",
    "This function implements the posterior mean estimator for situations where prior probabilities are uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pme0(q, r, d, tau):\n",
    "    \"\"\"Posterior mean estimator (PME)\n",
    "    \n",
    "    Args:\n",
    "        q (float): Prior probability\n",
    "        r (float): Effective observation\n",
    "        d (float): Signal amplitude\n",
    "        tau (float): Standard deviation of noise\n",
    "    Returns:\n",
    "        sHat (float): Probability s is one\n",
    "    \n",
    "    \"\"\"\n",
    "    sHat = ( q*np.exp( -(r-d)**2 / (2*(tau**2)) ) \\\n",
    "            / ( q*np.exp( -(r-d)**2 / (2*(tau**2))) + (1-q)*np.exp( -r**2 / (2*(tau**2))) ) ).astype(float)\n",
    "    return sHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Denoiser\n",
    "\n",
    "This function performs believe propagation (BP) on the factor graph of the outer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicDenoiser1(r,G,messageBlocks,L,M,K,tau,d,numBPiter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r (float): Effective observation\n",
    "        d (float): Signal amplitude\n",
    "        tau (float): Standard deviation of noise\n",
    "    \"\"\"\n",
    "    p0 = 1-(1-1/M)**K\n",
    "    p1 = p0*np.ones(r.shape,dtype=float)\n",
    "    mu = np.zeros(r.shape,dtype=float)\n",
    "\n",
    "    # Compute local estimate (lambda) based on effective observation using PME.\n",
    "    localEstimates = pme0(p0, r, d, tau)\n",
    "    \n",
    "    # Reshape local estimate (lambda) into an LxM matrix\n",
    "    Beta = localEstimates.reshape(L,-1)\n",
    "    for i in range(L):\n",
    "        Beta[i,:] = approximateVector(Beta[i,:], K)\n",
    "\n",
    "    # There is an issue BELOW for numBPiter greater than one!\n",
    "    for iter in range(numBPiter):    \n",
    "        # Rotate PME 180deg about y-axis\n",
    "        Betaflipped = np.hstack((Beta[:,0].reshape(-1,1),np.flip(Beta[:,1:],axis=1)))\n",
    "        # Compute and store all FFTs\n",
    "        BetaFFT = np.fft.fft(Beta)\n",
    "        BetaflippedFFT = np.fft.fft(Betaflipped)\n",
    "        for i in range(L):\n",
    "            if messageBlocks[i]:\n",
    "                # Parity sections connected to info section i\n",
    "                parityIndices = np.where(G[i])[0]   # Identities of parity block(s) attached\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                for j in parityIndices:  # Compute message for check associated with parity j\n",
    "                    # Other info blocks connected to this parity block\n",
    "                    messageIndices = np.setdiff1d(np.where(G[j])[0],i)  ## all indicies attahced to j, other than i\n",
    "                    BetaFFTprime = np.vstack((BetaFFT[j],BetaflippedFFT[messageIndices,:]))  ## j is not part of G[j]\n",
    "                    # Multiply the relevant FFTs\n",
    "                    BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                    # IFFT\n",
    "                    BetaIFFTprime1 = np.fft.ifft(BetaFFTprime).real # multiple parity\n",
    "                    BetaIFFTprime = np.vstack((BetaIFFTprime,BetaIFFTprime1)) if BetaIFFTprime.size else BetaIFFTprime1\n",
    "                    # need to stack from all parity\n",
    "                BetaIFFTprime = np.prod(BetaIFFTprime,axis=0) # pointwise product of distribution\n",
    "            else:\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                # Information sections connected to this parity section (assuming no parity over parity sections)\n",
    "                Indices = np.where(G[i])[0]\n",
    "                # FFT\n",
    "                BetaFFTprime = BetaFFT[Indices,:]\n",
    "                # Multiply the relevant FFTs\n",
    "                BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                # IFFT\n",
    "                BetaIFFTprime = np.fft.ifft(BetaFFTprime).real            \n",
    "            mu[i*M:(i+1)*M] = approximateVector(BetaIFFTprime, K).reshape(-1,1)\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicDenoiser2(r,OuterCode,K,tau,d,numBPiter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r (float): Effective observation\n",
    "        d (float): Signal amplitude\n",
    "        tau (float): Standard deviation of noise\n",
    "    \"\"\"\n",
    "    M = OuterCode.getsparseseclength()\n",
    "    L = OuterCode.getvarcount()\n",
    "\n",
    "    p0 = 1-(1-1/M)**K\n",
    "    p1 = p0*np.ones(r.shape,dtype=float)\n",
    "    mu = np.zeros(r.shape,dtype=float)\n",
    "\n",
    "    # Compute local estimate (lambda) based on effective observation using PME.\n",
    "    localEstimates = pme0(p0, r, d, tau)\n",
    "    \n",
    "    # Reshape local estimate (lambda) into an LxM matrix\n",
    "    Beta = localEstimates.reshape(L,-1)\n",
    "    OuterCode.reset()\n",
    "    for varnodeid in OuterCode.getvarlist():\n",
    "        i = varnodeid - 1\n",
    "        Beta[i,:] = approximateVector(Beta[i,:], K)\n",
    "        OuterCode.setobservation(varnodeid, Beta[i,:]) # CHECK\n",
    "    \n",
    "    for iter in range(1):    # CHECK: Leave at 1 for now\n",
    "        OuterCode.updatechecks()\n",
    "        OuterCode.updatevars()\n",
    "\n",
    "    for varnodeid in OuterCode.getvarlist():\n",
    "        i = varnodeid - 1\n",
    "        Beta[i,:] = OuterCode.getextrinsicestimate(varnodeid)\n",
    "        mu[i*M:(i+1)*M] = approximateVector(Beta[i,:], K).reshape(-1,1)\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but note we use `longdouble` types because the expentials are often too big to fit into a normal `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_state_update(z, s, P, L, M, Ab, Az, K, G, messageBlocks, denoiserType, numBPiter,OuterCode):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        s: State update through AMP composite iteration\n",
    "        z: Residual update through AMP composite iteration\n",
    "        tau (float): Standard deviation of noise\n",
    "        mu: Product of messages from adjoining factors\n",
    "    \"\"\"\n",
    "    n = z.size\n",
    "    d = np.sqrt(n*P/L)\n",
    "\n",
    "    # Compute tau online using the residual\n",
    "    tau = np.sqrt(np.sum(z**2)/n)\n",
    "\n",
    "    # Compute effective observation\n",
    "    r = (d*s + Az(z))\n",
    "\n",
    "    # Compute updated state\n",
    "    if denoiserType==0:\n",
    "        # Use the uninformative prior p0 for Giuseppe's scheme\n",
    "        p0 = 1-(1-1/M)**K\n",
    "        s = pme0(p0, r, d, tau)\n",
    "    elif denoiserType==1:\n",
    "        mu = dynamicDenoiser1(r,G,messageBlocks,L,M,K,tau,d,numBPiter)\n",
    "        s = pme0(mu, r, d, tau)\n",
    "    else:\n",
    "        mu = dynamicDenoiser2(r,OuterCode,K,tau,d,numBPiter)\n",
    "        s = pme0(mu, r, d, tau)\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_residual(y, z, s1, s2, d1, d2, Ab1, Ab2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        s1: State update through AMP composite iteration\n",
    "        s2: State update through AMP composite iteration\n",
    "        y: Original observation\n",
    "        tau (float): Standard deviation of noise\n",
    "    \"\"\"\n",
    "    n = y.size\n",
    "    \n",
    "    # Compute tau online using the residual\n",
    "    tau = np.sqrt(np.sum(z**2)/n)\n",
    "\n",
    "    # Compute residual\n",
    "    Onsager1 = d1*(np.sum(s1) - np.sum(s1**2))\n",
    "    Onsager2 = d2*(np.sum(s2) - np.sum(s2**2))   \n",
    "    z_plus = y - d1*Ab1(s1) - d2*Ab2(s2)+ (z/(n*tau**2))*(Onsager1 + Onsager2)\n",
    "    \n",
    "    return z_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree decoder\n",
    "\n",
    "This function implements the tree deocoder for a specific graph corresponding to the outer tree code\n",
    "\n",
    "It is currently hard-coded for a specfic architecture\n",
    "\n",
    "The architecture is based on a tri-adic design and can be found in the simulation results section of https://arxiv.org/pdf/2001.03705.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_decoder(cs_decoded_tx_message,G,L,J,B,listSize):\n",
    "    \n",
    "    tree_decoded_tx_message = np.empty(shape=(0,0))\n",
    "    \n",
    "    Paths012 = merge_pathst2(cs_decoded_tx_message[:,0:3])\n",
    "    Paths345 = merge_pathst2(cs_decoded_tx_message[:,3:6])    \n",
    "    Paths678 = merge_pathst2(cs_decoded_tx_message[:,6:9])\n",
    "    Paths91011 = merge_pathst2(cs_decoded_tx_message[:,9:12])    \n",
    "    Paths01267812 = merge_pathslevel2t2(Paths012,Paths678,cs_decoded_tx_message[:,[0,6,12]])    \n",
    "    Paths3459101115 = merge_pathslevel3t2(Paths345,Paths91011,cs_decoded_tx_message[:,[4,10,15]])   \n",
    "    Paths01267812345910111513 = merge_all_paths0t2(Paths01267812,Paths3459101115,cs_decoded_tx_message[:,[1,9,13]])\n",
    "    Paths = merge_all_paths_finalt2(Paths01267812345910111513,cs_decoded_tx_message[:,[3,7,14]])\n",
    "      \n",
    "    return Paths\n",
    "\n",
    "def merge_pathst2(A):\n",
    "    listSize = A.shape[0]\n",
    "    B = np.array([np.mod(A[:,0] + a,2**16) for a in A[:,1]]).flatten()\n",
    "     \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,listSize).reshape(-1,1),np.floor(I/listSize).reshape(-1,1)]).astype(int)\n",
    "            Paths = np.vstack((Paths,np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)]))) if Paths.size else np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)])\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_pathslevel2t2(Paths012,Paths678,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths0 = Paths012[:,0]\n",
    "    Paths6 = Paths678[:,0]\n",
    "    B = np.array([np.mod(A[Paths0,0] + a,2**16) for a in A[Paths6,1]]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths0.shape[0]).reshape(-1,1),np.floor(I/Paths0.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths012[I1[:,0]].reshape(-1,3),Paths678[I1[:,1]].reshape(-1,3),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "               \n",
    "    return Paths\n",
    "\n",
    "def merge_pathslevel3t2(Paths345,Paths91011,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths4 = Paths345[:,1]\n",
    "    Paths10 = Paths91011[:,1]\n",
    "    B = np.array([np.mod(A[Paths4,0] + a,2**16) for a in A[Paths10,1]]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths4.shape[0]).reshape(-1,1),np.floor(I/Paths4.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths345[I1[:,0]].reshape(-1,3),Paths91011[I1[:,1]].reshape(-1,3),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "\n",
    "def merge_all_paths0t2(Paths01267812,Paths3459101115,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths1 = Paths01267812[:,1]\n",
    "    Paths9 = Paths3459101115[:,3]\n",
    "\n",
    "    B = np.array([np.mod(A[Paths1,0] + a,2**16) for a in A[Paths9,1]]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths1.shape[0]).reshape(-1,1),np.floor(I/Paths1.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths01267812[I1[:,0]].reshape(-1,7),Paths3459101115[I1[:,1]].reshape(-1,7),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_all_paths_finalt2(Paths01267812345910111513,A):\n",
    "    \n",
    "    listSize = A.shape[0]\n",
    "    Paths3 = Paths01267812345910111513[:,7]\n",
    "    Paths7 = Paths01267812345910111513[:,4]\n",
    "    B = np.mod(A[Paths3,0] + A[Paths7,1] ,2**16)\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            PPaths = np.hstack((Paths01267812345910111513[I].reshape(-1,15),np.repeat(i,I.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    return Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tree decoder outputs more than $K$ valid paths, retain $K-\\delta$ of them based on their LLRs\n",
    "\n",
    "$\\delta$ is currently set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_topKminusdelta_paths(Paths, cs_decoded_tx_message, s, J,K,delta):\n",
    "    \n",
    "    L1 = Paths.shape[0]\n",
    "    LogL = np.zeros((L1,1))\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,j*(2**J)+cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else j*(2**J)+cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        LogL[i] = np.sum(np.log(s[msg_bit])) \n",
    "    Indices =  LogL.reshape(1,-1).argsort()[0,-(K-delta):]\n",
    "    Paths = Paths[Indices,:].reshape(((K-delta),-1))\n",
    "    \n",
    "    return Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5 # Number of active users\n",
    "B=128 # Payload size of each active user\n",
    "L=16 # Number of sections/sub-blocks\n",
    "n=38400 # Total number of channel uses (real d.o.f)\n",
    "T=12 # Number of AMP iterations\n",
    "listSize = 2*K  # List size retained for each section after AMP converges\n",
    "J=16  # Length of each coded sub-block\n",
    "M=2**J # Length of each section\n",
    "messageBlocks = np.array([1,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0]).astype(int) # Indicates the indices of information blocks\n",
    "# Adjacency matrix of the outer code/graph\n",
    "G = np.zeros((L,L)).astype(int)\n",
    "# G contains info on what parity blocks a message is attached to and what message blocks a parity is involved with\n",
    "# Currently, we do not allow parity over parities. BP code needs to be modified a little to accomodate parity over parities\n",
    "G[0,[2,12]]=1\n",
    "G[1,[2,13]]=1\n",
    "G[2,[0,1]]=1\n",
    "G[3,[5,14]]=1\n",
    "G[4,[5,15]]=1\n",
    "G[5,[3,4]]=1\n",
    "G[6,[8,12]]=1\n",
    "G[7,[8,14]]=1\n",
    "G[8,[6,7]]=1\n",
    "G[9,[11,13]]=1\n",
    "G[10,[11,15]]=1\n",
    "G[11,[9,10]]=1\n",
    "G[12,[0,6]]=1\n",
    "G[13,[1,9]]=1\n",
    "G[14,[3,7]]=1\n",
    "G[15,[4,10]]=1\n",
    "denoiserType = 1 # Select denoiser: 0 - Original PME; 1 - Dynamic PME; 2+ - Natrual BP.\n",
    "numBPiter = 1; # Number of BP iterations on outer code. 1 seems to be good enough & AMP theory including state evolution valid only for one BP iteration\n",
    "EbNodB = 2.2 # Energy per bit. With iterative extension, operating EbN0 falls to 2.05 dB for 25 users with 1 round SIC\n",
    "delta = 0\n",
    "simCount = 10 # number of simulations\n",
    "\n",
    "# EbN0 in linear scale\n",
    "EbNo = 10**(EbNodB/10)\n",
    "P = 2*B*EbNo/n\n",
    "σ_n = 1\n",
    "#Generate the power allocation and set of tau coefficients\n",
    "\n",
    "# We assume equal power allocation for all the sections. Code has to be modified a little to accomodate non-uniform power allocations\n",
    "Phat = n*P/L\n",
    "d = np.sqrt(n*P/L)\n",
    "\n",
    "# msgDetected0=0\n",
    "msgDetected1=0\n",
    "msgDetected2=0\n",
    "\n",
    "for simIndex in range(simCount):\n",
    "    print('Simulation Number: ' + str(simIndex))\n",
    "    \n",
    "    # Generate active users message sequences\n",
    "    tx_message = np.random.randint(2, size=(K,B))\n",
    "\n",
    "    # Outer-encode the message sequences\n",
    "    encoded_tx_message_indices = Tree_encode(tx_message,K,messageBlocks,G,L,J)\n",
    "    codewords = OuterCode1.encodemessages(tx_message)\n",
    "\n",
    "    # Convert indices to sparse representation\n",
    "    # sTrue: True state\n",
    "    sTrue1 = convert_indices_to_sparse(encoded_tx_message_indices, L, J, K)\n",
    "    sTrue2 = np.sum(codewords,axis=0).reshape(-1,1)\n",
    "\n",
    "    # Generate the binned SPARC codebook\n",
    "    Ab, Az = sparc_codebook(L, M, n, P)\n",
    "    \n",
    "    # Generate our transmitted signal X\n",
    "    x1 = np.sqrt(Phat)*Ab(sTrue1)\n",
    "    x2 = np.sqrt(Phat)*Ab(sTrue2)\n",
    "    \n",
    "    # Generate random channel noise and thus also received signal y\n",
    "    z = np.random.randn(n, 1) * σ_n\n",
    "    y1 = (x1 + z).reshape(-1, 1)\n",
    "    y2 = (x2 + z).reshape(-1, 1)\n",
    "\n",
    "    # Run AMP decoding\n",
    "    z1 = y1.copy()\n",
    "    z2 = y2.copy()\n",
    "    s0 = np.zeros((L*M, 1)) # No interference in dexing code below\n",
    "    s1 = np.zeros((L*M, 1))\n",
    "    s2 = np.zeros((L*M, 1))\n",
    "\n",
    "    for t in range(T):\n",
    "        s1 = amp_state_update(z1, s1, P, L, M, Ab, Az, K, G, messageBlocks, 1, numBPiter,OuterCode1)\n",
    "        s2 = amp_state_update(z2, s2, P, L, M, Ab, Az, K, G, messageBlocks, 2, numBPiter,OuterCode1)\n",
    "\n",
    "\n",
    "        z1 = amp_residual(y1, z1, s1, s0, d, d, Ab, Ab)\n",
    "        z2 = amp_residual(y2, z2, s2, s0, d, d, Ab, Ab)\n",
    "\n",
    "    \n",
    "    # Convert decoded sparse vector into vector of indices  \n",
    "    cs_decoded_tx_message1 = convert_sparse_to_indices(s1, L, J, listSize)\n",
    "\n",
    "    # Tree decoder to decode individual messages from lists output by AMP\n",
    "    Paths1 = Tree_decoder(cs_decoded_tx_message1,G,L,J,B,listSize)\n",
    "    \n",
    "    # Re-align paths to the correct order\n",
    "    perm1 = np.argsort(np.array([0,1,2,6,7,8,12,3,4,5,9,10,11,15,13,14]))\n",
    "    Paths1 = Paths1[:,perm1]\n",
    "    \n",
    "    # If tree deocder outputs more than K valid paths, retain only K of them\n",
    "    if Paths1.shape[0] > K:\n",
    "        Paths1 = pick_topKminusdelta_paths(Paths1, cs_decoded_tx_message1, s1, J, K,0)\n",
    "\n",
    "    # Extract the message indices from valid paths in the tree    \n",
    "    Tree_decoded_indices1 = extract_msg_indices(Paths1,cs_decoded_tx_message1, L, J)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Decoding wiht Graph\n",
    "    originallist = codewords.copy()\n",
    "    recoveredcodewords = FGG.decoder(OuterCode1,s2,2*K)\n",
    "\n",
    "    # Calculation of per-user prob err\n",
    "    simMsgDetected1 = 0\n",
    "    simMsgDetected2 = 0\n",
    "    for i in range(K):\n",
    "        simMsgDetected1 = simMsgDetected1 + np.equal(encoded_tx_message_indices[i,:],Tree_decoded_indices1).all(axis=1).any()\n",
    "    matches = FGG.numbermatches(originallist,recoveredcodewords)\n",
    "    \n",
    "    print('Group 1: ' + str(simMsgDetected1) + ' out of ' + str(K))\n",
    "    print('Group 2: ' + str(matches) + ' out of ' + str(K))\n",
    "    msgDetected1 = msgDetected1 + simMsgDetected1\n",
    "    msgDetected2 = msgDetected2 + matches\n",
    "    \n",
    "errorRate1= (K*simCount - msgDetected1)/(K*simCount)\n",
    "errorRate2= (K*simCount - msgDetected2)/(K*simCount)\n",
    "\n",
    "print(\"Per user probability of error (Group 1) = \", errorRate1)\n",
    "print(\"Per user probability of error (Group 2) = \", errorRate2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
